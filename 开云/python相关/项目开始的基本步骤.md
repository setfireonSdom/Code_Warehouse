# 创建虚拟环境  
1️⃣ 确认你的 Python 版本 
```
python3 --version
```  
2️⃣ 在你的项目目录下新建 venv  
```
python3 -m venv .venv
```  
3️⃣ 激活虚拟环境（非常重要）  
```
source .venv/bin/activate
```
# 安装包流程(以ollama项目为例，遇到pandas导入报错)  
> AttributeError: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)  

1️⃣ 升级 pip（避免旧 pip 装坏包）
```
pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple/
```
2️⃣ 安装 pandas（先验证它）
```
pip install pandas
```  
测试  
```
python - <<'EOF'
import pandas
print("pandas version:", pandas.__version__)
print("pandas path:", pandas.__file__)
EOF
```
✅ 如果这一步成功，说明环境是健康的。  
3️⃣ 安装 LlamaIndex + Ollama 插件
```
pip install \
  llama-index \
  llama-index-core \
  llama-index-llms-ollama \
  llama-index-embeddings-olla
```
# 案例
导入包，不同平台，Provider不一样。  比如本地做RAG，使用Ollama和OllamaEmbedding。
```
# ========= 导入依赖 =========
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

import logging
import os
from llama_index.core import Settings

Settings.chunk_size = 512
Settings.chunk_overlap = 50
```
接下来问答  
```
logging.basicConfig(level=logging.ERROR)

print("正在解析文件...")
documents = SimpleDirectoryReader("./docs").load_data()

print("正在创建索引...")
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=OllamaEmbedding(
        model_name="nomic-embed-text",  # 本地 embedding 模型
        base_url="http://localhost:11434"
    )
)

print("正在创建提问引擎...")
query_engine = index.as_query_engine(
    streaming=True,
    llm=Ollama(
        model="llama3.2:latest",               # 本地生成模型
        base_url="http://localhost:11434",
        request_timeout=120
    )
)

print("正在生成回复...")
streaming_response = query_engine.query('小明讨厌火锅和雪糕，对吗？')
print("回答是：")
# 采用流式输出
streaming_response.print_response_stream()
```
我在`./docs`文件夹下加入了一个.md文件，里面写了：
```
小明喜欢吃火锅。
但是小明最喜欢吃雪糕。
```
模型的回答是：
```
不是
```
