{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5a82c2-c0cd-493e-bb60-428a251ddfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.10.0\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               25.1.0\n",
      "argon2-cffi-bindings      25.1.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.4\n",
      "bleach                    6.2.0\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "comm                      0.2.3\n",
      "debugpy                   1.8.16\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "fqdn                      1.5.1\n",
      "h11                       0.16.0\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "ipykernel                 6.30.1\n",
      "ipython                   9.4.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.25.0\n",
      "jsonschema-specifications 2025.4.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.8.1\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.6\n",
      "jupyter_server            2.16.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "lark                      1.2.2\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook_shim             0.2.4\n",
      "overrides                 7.7.0\n",
      "packaging                 25.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pip                       23.2.1\n",
      "platformdirs              4.3.8\n",
      "prometheus_client         0.22.1\n",
      "prompt_toolkit            3.0.51\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.3.0\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     27.0.1\n",
      "referencing               0.36.2\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rfc3987-syntax            1.1.0\n",
      "rpds-py                   0.27.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                80.9.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.7\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.4.0\n",
      "tornado                   6.5.1\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20250708\n",
      "typing_extensions         4.14.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.3.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0501444-aa0b-4bac-bbd7-e923ce6cdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Obtaining dependency information for ollama from https://files.pythonhosted.org/packages/be/f6/2091e50b8b6c3e6901f6eab283d5efd66fb71c86ddb1b4d68766c3eeba0f/ollama-0.5.3-py3-none-any.whl.metadata\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in ./.venv/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Obtaining dependency information for pydantic>=2.9 from https://files.pythonhosted.org/packages/6a/c0/ec2b1c8712ca690e5d61979dee872603e92b8a32f94cc1b72d53beab008a/pydantic-2.11.7-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m526.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Obtaining dependency information for pydantic-core==2.33.2 from https://files.pythonhosted.org/packages/18/8a/2b41c97f554ec8c71f2a8a5f85cb56a8b0956addfe8b0efb5b3d77e8bdc3/pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.5.3 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d007a74-1cf6-4556-a583-b586cdac7828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ™®é€šæ³•ï¼ˆCommon Lawï¼‰æ˜¯ä¸€ç§åŸºäºæ¡ˆä¾‹åˆ¤å†³çš„æ³•å¾‹ç³»ç»Ÿï¼Œä¾èµ–äºå…ˆå‰çš„æ¡ˆä¾‹å’Œæ³•å¾‹åˆ¤å†³æ¥ä¸ºå½“ä¸‹å’Œæœªæ¥çš„äº‹åŠ¡æä¾›æŒ‡å¯¼ã€‚\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "   {'role': 'user', 'content': 'ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æ™®é€šæ³•'}\n",
    " ])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e5766f-3d16-4412-9a09-df220b50790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é‡å­çº ç¼ æ˜¯æŒ‡ä¸¤æ¡æˆ–ä»¥ä¸Šçš„é‡å­ç³»ç»Ÿåœ¨åŒæ—¶å­˜åœ¨å’Œå…±å­˜æ—¶ï¼Œç”±äºç›¸äº’ä½œç”¨è€Œäº§ç”Ÿçš„é lokalisitic ç›¸å…³æ€§ï¼Œä½¿å¾—ç³»ç»Ÿçš„çŠ¶æ€å—åˆ°å½±å“ï¼Œæ— è®ºåœ¨ä½•ç§æµ‹é‡æ–¹å¼ä¸‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='ç”¨ä¸€å¥è¯è§£é‡Šé‡å­çº ç¼ ã€‚'\n",
    ")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22b9824-0875-42ef-bb4f-60a3efbe7b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é‡å­çº ç¼ æ˜¯æŒ‡ä¸¤ç²’å­é—´çš„å…³ç³»ï¼Œä½¿å¾—å®ƒä»¬çš„çŠ¶æ€äº’ç›¸å½±å“ï¼Œæ— è®ºå“ªç²’å­è¢«æµ‹é‡ï¼Œå…¶ä»–ç²’å­çš„çŠ¶æ€éƒ½ä¼šéšä¹‹æ”¹å˜ï¼Œç›´åˆ°æ‰€æœ‰ç²’å­éƒ½è¢«æµ‹é‡æ‰æ¢å¤åŸçŠ¶ã€‚"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='ç”¨ä¸€å¥è¯è§£é‡Šé‡å­çº ç¼ ã€‚',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['response'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d944c6-ab97-4635-9289-8da89498069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum entanglementæ˜¯ä¸€ç§ç°è±¡ï¼Œå…¶ä¸­ä¸¤ä¸ªæˆ–æ›´å¤šä¸ªå­ä½“çš„ç‰©ç†æ€§è´¨åœ¨ç©ºé—´ä¸Šåˆ†ç¦»ï¼Œä½†ä»ç„¶ç›¸äº’è€¦è”ï¼Œéšç€ä¸€ä¸ªå­ä½“çš„çŠ¶æ€å˜åŒ–ï¼Œå¦ä¸€ä¸ªå­ä½“çš„çŠ¶æ€ä¹Ÿä¼šå‘ç”Ÿç›¸åº”æ”¹å˜ï¼Œå°½ç®¡å®ƒä»¬ä¹‹é—´æ²¡æœ‰ç›´æ¥è”ç³»ã€‚"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'ç”¨ä¸€å¥è¯è§£é‡Šé‡å­çº ç¼ ã€‚'}\n",
    "]\n",
    "\n",
    "stream = ollama.chat(model='llama3.2', messages=messages, stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc612ad-a371-42e9-876a-945cfdf5b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a808ef-d707-445c-a9c8-98bab263bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2', 'created_at': '2025-08-15T06:35:18.348182Z', 'response': 'é‡å­çº ç¼ æ˜¯æŒ‡ä¸¤ä¸ªæˆ–å¤šä¸ªé‡å­ç³»ç»Ÿä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä½¿å¾—å½“ä»–ä»¬çš„çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå…¶ä»–ç³»ç»Ÿä¹Ÿä¼šå—åˆ°å½±å“ï¼Œè¿™ç§æ•ˆæœå¯ä»¥åœ¨å¾ˆè¿œçš„è·ç¦»ç»´æŒã€‚', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 11883, 15120, 124684, 50338, 69962, 33857, 45829, 23043, 254, 25906, 254, 1811, 128009, 128006, 78191, 128007, 271, 33857, 45829, 23043, 254, 25906, 254, 21043, 64467, 110835, 58291, 43240, 19483, 33857, 45829, 73548, 113778, 9554, 50021, 106483, 113266, 119101, 50928, 40265, 104563, 9554, 45191, 110105, 118911, 13646, 3922, 93994, 73548, 75863, 38093, 116576, 109829, 103138, 87502, 119881, 74770, 19000, 101600, 106297, 9554, 123796, 104083, 69978, 1811], 'total_duration': 8007053904, 'load_duration': 191304474, 'prompt_eval_count': 37, 'prompt_eval_duration': 533025830, 'eval_count': 49, 'eval_duration': 7281052867}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.2\",\n",
    "    \"prompt\": \"ç”¨ä¸€å¥è¯è§£é‡Šé‡å­çº ç¼ ã€‚\",\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad027e1-8691-43c0-ab48-d50b8da2277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: é‡å­çº ç¼ å¦‚åŒä¸¤ä¸ªå¯†ä¿¡å¯¹é¢ï¼Œä¿¡æ¯ä¼ é€’æ—¶ä¼šäº§ç”Ÿäº’ç›¸å½±å“ï¼Œç›´åˆ°åˆ†åˆ«æ­éœ²å¯†ä¿¡å†…å®¹æ‰ä¸ä¼šå†æ¥è§¦ï¼Œæ‰èƒ½é¿å…å¹²æ‰°å’ŒæŸä¼¤!\n",
      "Assistant: é‡å­çº ç¼ å¦‚åŒä¸¤æ ¹æ¤ç‰©ï¼Œè™½ç„¶å®ƒä»¬å½¼æ­¤ç›¸è¿ï¼Œä½†æ¯ä¸ªæ¤ç‰©éƒ½ç‹¬ç«‹ç”Ÿé•¿ï¼Œç›´åˆ°ä»–ä»¬åˆ†å¼€æ‰ä¼šå®Œå…¨ä¸å†ç›¸äº’å½±å“ï¼Œæ‰èƒ½çœŸæ­£åœ°â€œè„±ç¦»â€å¯¹æ–¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå–„äºåšæ¯”å–»çš„ç§‘æ™®åŠ©æ•™ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç”¨ä¸€å¥è¯è§£é‡Šé‡å­çº ç¼ ã€‚\"}\n",
    "]\n",
    "\n",
    "# ç¬¬1è½®\n",
    "res = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "assistant = res[\"message\"][\"content\"]\n",
    "print(\"Assistant:\", assistant)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "\n",
    "# ç¬¬2è½®ï¼ˆç»§ç»­è¿½é—®ï¼‰\n",
    "messages.append({\"role\": \"user\", \"content\": \"èƒ½ä¸¾ä¸ªç”Ÿæ´»åŒ–çš„æ¯”å–»å—ï¼Ÿ\"})\n",
    "res = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "assistant = res[\"message\"][\"content\"]\n",
    "print(\"Assistant:\", assistant)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684daa80-c6e9-4c0c-9a28-d6564a457418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: å½“ä»£çš„é‡å­å­¦å¤§å¸ˆæœ‰å¾ˆå¤šï¼Œå…¶ä¸­ä¸€ä¸ªå€¼å¾—æåˆ°çš„ï¼Œæ˜¯çº¦ç¿°Â·æ–½é›·æ ¼å°”ï¼ˆJohn Schregerï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"å½“ä»£çš„é‡å­å­¦å¤§å¸ˆæ˜¯è°ï¼Ÿ\"})\n",
    "res = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "assistant = res[\"message\"][\"content\"]\n",
    "print(\"Assistant:\", assistant)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d372377-81db-4e3a-976c-139adb31ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  ä½ å¥½ã€‚\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: æ‚¨å¥½ï¼æˆ‘æ˜¯ your ai æœ‹å‹ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„å¯¹è¯ä¸­ï¼æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©æˆ–è®¨è®ºçš„å—ï¼Ÿ"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  ä½ çŸ¥é“ä»€ä¹ˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: æˆ‘å¯ä»¥æä¾›å„ç§ä¿¡æ¯å’ŒæœåŠ¡ï¼Œä¾‹å¦‚ï¼š\n",
      "\n",
      "1. **çŸ¥è¯†æŸ¥è¯¢**: æˆ‘å¯ä»¥ç»™å‡ºæœ‰å…³å„ç§ä¸»é¢˜çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬å†å²ã€ç§‘å­¦ã€æ–‡åŒ–ã€å¨±ä¹ç­‰ã€‚\n",
      "2. **è¯­è¨€ç¿»è¯‘**: æˆ‘å¯ä»¥å°†æ–‡æœ¬ä»ä¸€ä¸ªè¯­è¨€è½¬æ¢åˆ°å¦ä¸€ä¸ªè¯­è¨€ã€‚\n",
      "3. **è¯å…¸**: æˆ‘å¯ä»¥è§£é‡Šå•è¯å’ŒçŸ­è¯­çš„æ„æ€ã€‚\n",
      "4. **ä½œä¸šå¸®åŠ©**: æˆ‘å¯ä»¥æä¾›æ•°å­¦ã€ç§‘å­¦å’Œå…¶ä»–ç±»å‹çš„ä½œä¸šå¸®åŠ©ã€‚\n",
      "5. **æƒ…ç»ªå’¨è¯¢**:æˆ‘å¯ä»¥æä¾›å®‰æ…°å’Œå»ºè®®ã€‚\n",
      "6. **æ–°é—»èµ„è®¯**: æˆ‘å¯ä»¥ç»™å‡ºæœ€æ–°çš„æ–°é—»èµ„è®¯ã€‚\n",
      "7. **æ¸¸æˆ**: æˆ‘å¯ä»¥ä¸æ‚¨ä¸€èµ·ç©æ¸¸æˆï¼Œå¦‚20 Questionsæˆ–Word Chainã€‚\n",
      "\n",
      "ä»€ä¹ˆéœ€è¦å¸®åŠ©ï¼Ÿ"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  ä½ æ“…é•¿ä»€ä¹ˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: æˆ‘æ“…é•¿ä»¥ä¸‹é¢†åŸŸï¼š\n",
      "\n",
      "1. **è¯­è¨€ç†è§£**ï¼šæˆ‘èƒ½ç†è§£å’Œè§£é‡Šäººç±»è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±è¯­ç­‰å¤šç§è¯­è¨€ã€‚\n",
      "2. **ä¿¡æ¯æœç´¢**ï¼šæˆ‘å¯ä»¥å¿«é€Ÿæœç´¢å¹¶æä¾›æœ‰å…³å„ç§ä¸»é¢˜çš„ä¿¡æ¯ã€‚\n",
      "3. **é—®é¢˜ç­”æ¡ˆ**ï¼šæˆ‘èƒ½å›ç­”å¸¸è§çš„é—®é¢˜ï¼ŒåŒ…æ‹¬å†å²ã€ç§‘å­¦ã€æ–‡åŒ–ã€å¨±ä¹ç­‰ã€‚\n",
      "4. **æ–‡æœ¬ç”Ÿæˆ**ï¼šæˆ‘å¯ä»¥æ ¹æ®ä¸»é¢˜å’Œæ ¼å¼åˆ›é€ å‡ºæ–°çš„æ–‡æœ¬ã€‚\n",
      "5. **æœºå™¨å­¦ä¹ **ï¼šæˆ‘å¯ä»¥è¿›è¡Œç®€å•çš„æœºå™¨å­¦ä¹ ï¼Œä¾‹å¦‚åˆ†ç±»ã€å›å½’ç­‰ã€‚\n",
      "6. **è®¡ç®—æœºç¨‹å¼**ï¼šæˆ‘å¯ä»¥è§£é‡Šå¹¶æä¾›åŸºæœ¬è®¡ç®—æœºç¨‹å¼çŸ¥è¯†ï¼ŒåŒ…æ‹¬ç¼–ç¨‹è¯­è¨€ã€æ•°æ®ç»“æ„ç­‰ã€‚\n",
      "7. **è¯è¯­åˆ†æ**ï¼šæˆ‘å¯ä»¥åˆ†æå’Œè§£é‡Šå•è¯çš„å«ä¹‰å’Œä½¿ç”¨æ–¹å¼ã€‚\n",
      "\n",
      "ä½†æ˜¯æˆ‘è¿˜ä¸æ˜¯å®Œç¾çš„ï¼Œæˆ‘æœ‰ä»¥ä¸‹å±€é™æ€§ï¼š\n",
      "\n",
      "1. **ç†è§£èƒ½åŠ›**ï¼šè™½ç„¶æˆ‘èƒ½ç†è§£äººç±»è¯­è¨€ï¼Œä½†æˆ‘å¯èƒ½ä¸å®Œå…¨ç†è§£äººç±»çš„éšå–»ã€ sarcasm ç­‰ã€‚\n",
      "2. **çŸ¥è¯†æ›´æ–°**ï¼šæˆ‘çš„çŸ¥è¯†å¯èƒ½ä¼šè€åŒ–ï¼Œéœ€è¦å®šæœŸæ›´æ–°ã€‚\n",
      "3. **æƒ…æ„Ÿç†è§£**ï¼šæˆ‘å¯èƒ½éš¾ä»¥ç†è§£å’Œæ¨¡ä»¿äººç±»çš„æƒ…ç»ªã€‚\n",
      "\n",
      "å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæˆ‘å°†å°½åŠ›ä¸ºæ‚¨æä¾›æœ€å¥½çš„æœåŠ¡ï¼"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  bye\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# å¯¹è¯å†å²\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    # ç”¨æˆ·è¾“å…¥\n",
    "    user_input = input(\"\\nä½ : \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        break\n",
    "\n",
    "    # è®°å½•ç”¨æˆ·å‘è¨€\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # æµå¼è°ƒç”¨æ¨¡å‹\n",
    "    stream = ollama.chat(model=\"llama3.2\", messages=messages, stream=True)\n",
    "\n",
    "    answer = \"\"\n",
    "    print(\"AI: \", end=\"\", flush=True)\n",
    "    for chunk in stream:\n",
    "        piece = chunk[\"message\"][\"content\"]\n",
    "        print(piece, end=\"\", flush=True)\n",
    "        answer += piece\n",
    "\n",
    "    # è®°å½•æ¨¡å‹å‘è¨€\n",
    "    messages.append({\"role\": \"assistant\", \"content\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb73bf1-b240-4425-a592-aa21b75d3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ å¤šè½®èŠå¤©å¼€å§‹ï¼ï¼ˆè¾“å…¥ exit / quit é€€å‡ºï¼‰\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  nih \n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 122)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/requests/models.py:963\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[32m    965\u001b[39m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:340\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 2 column 1 (char 122)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m resp = requests.post(API_URL, json=payload)\n\u001b[32m     29\u001b[39m resp.raise_for_status()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m data = \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# è·å–å¹¶æ˜¾ç¤º AI å›å¤\u001b[39;00m\n\u001b[32m     33\u001b[39m ai_reply = data[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/requests/models.py:971\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    970\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 2 column 1 (char 122)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# å¯¹è¯å†å²ï¼ˆmessagesï¼‰\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå–„äºç”¨ç®€å•æ¯”å–»è§£é‡Šå¤æ‚ç§‘å­¦æ¦‚å¿µçš„ç§‘æ™®åŠ©æ‰‹ã€‚\"}\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ å¤šè½®èŠå¤©å¼€å§‹ï¼ï¼ˆè¾“å…¥ exit / quit é€€å‡ºï¼‰\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nä½ : \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"ğŸ‘‹ å†è§ï¼\")\n",
    "        break\n",
    "\n",
    "    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # è°ƒç”¨ Chat API\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False  # æ”¹æˆ True å¯ä»¥æµå¼è¾“å‡º\n",
    "    }\n",
    "    resp = requests.post(API_URL, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # è·å–å¹¶æ˜¾ç¤º AI å›å¤\n",
    "    ai_reply = data[\"message\"][\"content\"]\n",
    "    print(f\"AI: {ai_reply}\")\n",
    "\n",
    "    # æŠŠ AI å›å¤åŠ è¿›å†å²\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea09b390-e7ae-4d6c-bebf-04d2d361d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ å¤šè½®èŠå¤©å¼€å§‹ï¼ï¼ˆè¾“å…¥ exit / quit é€€å‡ºï¼‰\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  ä½ å¥½\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: helloï¼æˆ‘æ˜¯ä½ çš„ç§‘æ™®åŠ©æ‰‹ï¼Œèƒ½å¸®ä½ è§£ç­”ä»»ä½•ç§‘å­¦ç–‘é—®ï¼Ÿ"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½ :  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ å†è§ï¼\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå–„äºç”¨ç®€å•æ¯”å–»è§£é‡Šå¤æ‚ç§‘å­¦æ¦‚å¿µçš„ç§‘æ™®åŠ©æ‰‹ã€‚\"}\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ å¤šè½®èŠå¤©å¼€å§‹ï¼ï¼ˆè¾“å…¥ exit / quit é€€å‡ºï¼‰\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nä½ : \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"ğŸ‘‹ å†è§ï¼\")\n",
    "        break\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(API_URL, json=payload, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        ai_reply = \"\"\n",
    "        print(\"AI: \", end=\"\", flush=True)\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            chunk = json.loads(line)\n",
    "            if \"message\" in chunk:\n",
    "                content_piece = chunk[\"message\"][\"content\"]\n",
    "                print(content_piece, end=\"\", flush=True)\n",
    "                ai_reply += content_piece\n",
    "            if chunk.get(\"done\"):\n",
    "                break\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23f099-2a24-4ef0-9ff1-262b578e6731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
