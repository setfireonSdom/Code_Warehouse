{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5a82c2-c0cd-493e-bb60-428a251ddfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.10.0\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               25.1.0\n",
      "argon2-cffi-bindings      25.1.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.4\n",
      "bleach                    6.2.0\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "comm                      0.2.3\n",
      "debugpy                   1.8.16\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "fqdn                      1.5.1\n",
      "h11                       0.16.0\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "ipykernel                 6.30.1\n",
      "ipython                   9.4.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.25.0\n",
      "jsonschema-specifications 2025.4.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.8.1\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.6\n",
      "jupyter_server            2.16.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "lark                      1.2.2\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook_shim             0.2.4\n",
      "overrides                 7.7.0\n",
      "packaging                 25.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pip                       23.2.1\n",
      "platformdirs              4.3.8\n",
      "prometheus_client         0.22.1\n",
      "prompt_toolkit            3.0.51\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.3.0\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     27.0.1\n",
      "referencing               0.36.2\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rfc3987-syntax            1.1.0\n",
      "rpds-py                   0.27.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                80.9.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.7\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.4.0\n",
      "tornado                   6.5.1\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20250708\n",
      "typing_extensions         4.14.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.3.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0501444-aa0b-4bac-bbd7-e923ce6cdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Obtaining dependency information for ollama from https://files.pythonhosted.org/packages/be/f6/2091e50b8b6c3e6901f6eab283d5efd66fb71c86ddb1b4d68766c3eeba0f/ollama-0.5.3-py3-none-any.whl.metadata\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in ./.venv/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Obtaining dependency information for pydantic>=2.9 from https://files.pythonhosted.org/packages/6a/c0/ec2b1c8712ca690e5d61979dee872603e92b8a32f94cc1b72d53beab008a/pydantic-2.11.7-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m526.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9->ollama)\n",
      "  Obtaining dependency information for pydantic-core==2.33.2 from https://files.pythonhosted.org/packages/18/8a/2b41c97f554ec8c71f2a8a5f85cb56a8b0956addfe8b0efb5b3d77e8bdc3/pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->ollama)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "Successfully installed annotated-types-0.7.0 ollama-0.5.3 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d007a74-1cf6-4556-a583-b586cdac7828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通法（Common Law）是一种基于案例判决的法律系统，依赖于先前的案例和法律判决来为当下和未来的事务提供指导。\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "   {'role': 'user', 'content': '用一句话解释什么是普通法'}\n",
    " ])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e5766f-3d16-4412-9a09-df220b50790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量子纠缠是指两条或以上的量子系统在同时存在和共存时，由于相互作用而产生的非 lokalisitic 相关性，使得系统的状态受到影响，无论在何种测量方式下。\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='用一句话解释量子纠缠。'\n",
    ")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22b9824-0875-42ef-bb4f-60a3efbe7b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量子纠缠是指两粒子间的关系，使得它们的状态互相影响，无论哪粒子被测量，其他粒子的状态都会随之改变，直到所有粒子都被测量才恢复原状。"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='用一句话解释量子纠缠。',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['response'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d944c6-ab97-4635-9289-8da89498069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum entanglement是一种现象，其中两个或更多个子体的物理性质在空间上分离，但仍然相互耦联，随着一个子体的状态变化，另一个子体的状态也会发生相应改变，尽管它们之间没有直接联系。"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': '用一句话解释量子纠缠。'}\n",
    "]\n",
    "\n",
    "stream = ollama.chat(model='llama3.2', messages=messages, stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc612ad-a371-42e9-876a-945cfdf5b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a808ef-d707-445c-a9c8-98bab263bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2', 'created_at': '2025-08-15T06:35:18.348182Z', 'response': '量子纠缠是指两个或多个量子系统之间的相互作用，使得当他们的状态发生变化时，其他系统也会受到影响，这种效果可以在很远的距离维持。', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 11883, 15120, 124684, 50338, 69962, 33857, 45829, 23043, 254, 25906, 254, 1811, 128009, 128006, 78191, 128007, 271, 33857, 45829, 23043, 254, 25906, 254, 21043, 64467, 110835, 58291, 43240, 19483, 33857, 45829, 73548, 113778, 9554, 50021, 106483, 113266, 119101, 50928, 40265, 104563, 9554, 45191, 110105, 118911, 13646, 3922, 93994, 73548, 75863, 38093, 116576, 109829, 103138, 87502, 119881, 74770, 19000, 101600, 106297, 9554, 123796, 104083, 69978, 1811], 'total_duration': 8007053904, 'load_duration': 191304474, 'prompt_eval_count': 37, 'prompt_eval_duration': 533025830, 'eval_count': 49, 'eval_duration': 7281052867}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.2\",\n",
    "    \"prompt\": \"用一句话解释量子纠缠。\",\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad027e1-8691-43c0-ab48-d50b8da2277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 量子纠缠如同两个密信对面，信息传递时会产生互相影响，直到分别揭露密信内容才不会再接触，才能避免干扰和损伤!\n",
      "Assistant: 量子纠缠如同两根植物，虽然它们彼此相连，但每个植物都独立生长，直到他们分开才会完全不再相互影响，才能真正地“脱离”对方。\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个善于做比喻的科普助教。\"},\n",
    "    {\"role\": \"user\", \"content\": \"用一句话解释量子纠缠。\"}\n",
    "]\n",
    "\n",
    "# 第1轮\n",
    "res = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "assistant = res[\"message\"][\"content\"]\n",
    "print(\"Assistant:\", assistant)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "\n",
    "# 第2轮（继续追问）\n",
    "messages.append({\"role\": \"user\", \"content\": \"能举个生活化的比喻吗？\"})\n",
    "res = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "assistant = res[\"message\"][\"content\"]\n",
    "print(\"Assistant:\", assistant)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684daa80-c6e9-4c0c-9a28-d6564a457418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 当代的量子学大师有很多，其中一个值得提到的，是约翰·施雷格尔（John Schreger）。\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"当代的量子学大师是谁？\"})\n",
    "res = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "assistant = res[\"message\"][\"content\"]\n",
    "print(\"Assistant:\", assistant)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d372377-81db-4e3a-976c-139adb31ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  你好。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 您好！我是 your ai 朋友，欢迎来到我们的对话中！有什么需要帮助或讨论的吗？"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  你知道什么\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 我可以提供各种信息和服务，例如：\n",
      "\n",
      "1. **知识查询**: 我可以给出有关各种主题的信息，包括历史、科学、文化、娱乐等。\n",
      "2. **语言翻译**: 我可以将文本从一个语言转换到另一个语言。\n",
      "3. **词典**: 我可以解释单词和短语的意思。\n",
      "4. **作业帮助**: 我可以提供数学、科学和其他类型的作业帮助。\n",
      "5. **情绪咨询**:我可以提供安慰和建议。\n",
      "6. **新闻资讯**: 我可以给出最新的新闻资讯。\n",
      "7. **游戏**: 我可以与您一起玩游戏，如20 Questions或Word Chain。\n",
      "\n",
      "什么需要帮助？"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  你擅长什么\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 我擅长以下领域：\n",
      "\n",
      "1. **语言理解**：我能理解和解释人类语言，包括中文、英语等多种语言。\n",
      "2. **信息搜索**：我可以快速搜索并提供有关各种主题的信息。\n",
      "3. **问题答案**：我能回答常见的问题，包括历史、科学、文化、娱乐等。\n",
      "4. **文本生成**：我可以根据主题和格式创造出新的文本。\n",
      "5. **机器学习**：我可以进行简单的机器学习，例如分类、回归等。\n",
      "6. **计算机程式**：我可以解释并提供基本计算机程式知识，包括编程语言、数据结构等。\n",
      "7. **词语分析**：我可以分析和解释单词的含义和使用方式。\n",
      "\n",
      "但是我还不是完美的，我有以下局限性：\n",
      "\n",
      "1. **理解能力**：虽然我能理解人类语言，但我可能不完全理解人类的隐喻、 sarcasm 等。\n",
      "2. **知识更新**：我的知识可能会老化，需要定期更新。\n",
      "3. **情感理解**：我可能难以理解和模仿人类的情绪。\n",
      "\n",
      "如果您有任何问题或需要帮助，我将尽力为您提供最好的服务！"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  bye\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 对话历史\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    # 用户输入\n",
    "    user_input = input(\"\\n你: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        break\n",
    "\n",
    "    # 记录用户发言\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # 流式调用模型\n",
    "    stream = ollama.chat(model=\"llama3.2\", messages=messages, stream=True)\n",
    "\n",
    "    answer = \"\"\n",
    "    print(\"AI: \", end=\"\", flush=True)\n",
    "    for chunk in stream:\n",
    "        piece = chunk[\"message\"][\"content\"]\n",
    "        print(piece, end=\"\", flush=True)\n",
    "        answer += piece\n",
    "\n",
    "    # 记录模型发言\n",
    "    messages.append({\"role\": \"assistant\", \"content\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb73bf1-b240-4425-a592-aa21b75d3649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 多轮聊天开始！（输入 exit / quit 退出）\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  nih \n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 122)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/requests/models.py:963\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[32m    965\u001b[39m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:340\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 2 column 1 (char 122)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m resp = requests.post(API_URL, json=payload)\n\u001b[32m     29\u001b[39m resp.raise_for_status()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m data = \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 获取并显示 AI 回复\u001b[39;00m\n\u001b[32m     33\u001b[39m ai_reply = data[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/requests/models.py:971\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    970\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 2 column 1 (char 122)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# 对话历史（messages）\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个善于用简单比喻解释复杂科学概念的科普助手。\"}\n",
    "]\n",
    "\n",
    "print(\"💬 多轮聊天开始！（输入 exit / quit 退出）\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\n你: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"👋 再见！\")\n",
    "        break\n",
    "\n",
    "    # 添加用户消息到历史\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # 调用 Chat API\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False  # 改成 True 可以流式输出\n",
    "    }\n",
    "    resp = requests.post(API_URL, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # 获取并显示 AI 回复\n",
    "    ai_reply = data[\"message\"][\"content\"]\n",
    "    print(f\"AI: {ai_reply}\")\n",
    "\n",
    "    # 把 AI 回复加进历史\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea09b390-e7ae-4d6c-bebf-04d2d361d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 多轮聊天开始！（输入 exit / quit 退出）\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  你好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: hello！我是你的科普助手，能帮你解答任何科学疑问？"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "你:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 再见！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个善于用简单比喻解释复杂科学概念的科普助手。\"}\n",
    "]\n",
    "\n",
    "print(\"💬 多轮聊天开始！（输入 exit / quit 退出）\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\n你: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"👋 再见！\")\n",
    "        break\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    with requests.post(API_URL, json=payload, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        ai_reply = \"\"\n",
    "        print(\"AI: \", end=\"\", flush=True)\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            chunk = json.loads(line)\n",
    "            if \"message\" in chunk:\n",
    "                content_piece = chunk[\"message\"][\"content\"]\n",
    "                print(content_piece, end=\"\", flush=True)\n",
    "                ai_reply += content_piece\n",
    "            if chunk.get(\"done\"):\n",
    "                break\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23f099-2a24-4ef0-9ff1-262b578e6731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
