# 代码+解释
```
# 导入依赖
from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,StorageContext,load_index_from_storage
from llama_index.embeddings.dashscope import DashScopeEmbedding,DashScopeTextEmbeddingModels
from llama_index.llms.dashscope import DashScope
# 这两行代码是用于消除 WARNING 警告信息，避免干扰阅读学习，生产环境中建议根据需要来设置日志级别
import logging
logging.basicConfig(level=logging.ERROR)
from llama_index.llms.openai_like import OpenAILike
import os

def indexing(document_path="./docs", persist_path="knowledge_base/test"):
    """
    建立索引并持久化存储
    参数
      path(str): 文档路径
    """
    index = create_index(document_path)
    # 持久化索引，将索引保存为本地文件
    index.storage_context.persist(persist_path)

def create_index(document_path="./docs"):
    """
    建立索引
    参数
      path(str): 文档路径
    """
    # 解析 ./docs 目录下的所有文档
    documents = SimpleDirectoryReader(document_path).load_data()
    # 建立索引
    index = VectorStoreIndex.from_documents(
        documents,
        # 指定embedding 模型
        embed_model=DashScopeEmbedding(
            # 你也可以使用阿里云提供的其它embedding模型：https://help.aliyun.com/zh/model-studio/getting-started/models#3383780daf8hw
            model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2
        )
    )
    return index

def load_index(persist_path="knowledge_base/test"):
    """
    加载索引
    参数
      persist_path(str): 索引文件路径
    返回
      VectorStoreIndex: 索引对象
    """
    storage_context = StorageContext.from_defaults(persist_dir=persist_path)
    return load_index_from_storage(storage_context,embed_model=DashScopeEmbedding(
      model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2
    ))

def create_query_engine(index):
    """
    创建查询引擎
    参数
      index(VectorStoreIndex): 索引对象
    返回
      QueryEngine: 查询引擎对象
    """
    
    query_engine = index.as_query_engine(
      # 设置为流式输出
      streaming=True,
      # 此处使用qwen-plus-0919模型，你也可以使用阿里云提供的其它qwen的文本生成模型：https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u
      llm=OpenAILike(
          model="qwen-plus-0919",
          api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
          api_key=os.getenv("DASHSCOPE_API_KEY"),
          is_chat_model=True
          ))
    return query_engine

def ask(question, query_engine):
    """
    向答疑机器人提问
    参数
      question(str): 问题
      query_engine(QueryEngine): 查询引擎对象
    返回
      str: 回答
    """
    streaming_response = query_engine.query(question)
    streaming_response.print_response_stream()



from llama_index.core import PromptTemplate
def update_prompt_template(
        query_engine,
        qa_prompt_tmpl_str = (
        "你叫公司小蜜，是公司的答疑机器人。你需要仔细阅读参考信息，然后回答大家提出的问题。"
        "注意事项：\n"
        "1. 根据上下文信息而非先验知识来回答问题。\n"
        "2. 如果是工具咨询类问题，请务必给出下载地址链接。\n"
        "3. 如果员工部门查询问题，请务必注意有同名员工的情况，可能有2个、3个甚至更多同名的人\n"
        "以下是参考信息。"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "问题：{query_str}\n。"
        "回答："
    )):
    """
    修改prompt模板
    输入是prompt修改前的query_engine，以及提示词模板；输出是prompt修改后的query_engine
    """
    qa_prompt_tmpl_str = qa_prompt_tmpl_str
    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)
    query_engine.update_prompts(
        {"response_synthesizer:text_qa_template": qa_prompt_tmpl}
    )
    # print("提示词模板修改成功")
    return query_engine
```  
`from llama_index.llms.openai_like import OpenAILike`这行代码里导入的`OpenAILike`起到什么作用？  

它让“不是 OpenAI 官方的模型 / 接口”， 也能“伪装成 OpenAI 接口”， 从而直接复用 LlamaIndex（以及一大堆生态）里写死的 OpenAI 调用逻辑。  
  
OpenAILike 就是用来调用“接口长得像 OpenAI 的模型”的。只要模型“兼容 OpenAI 接口协议”，通常只需要更换 api_base / api_key / model 就可以用了。  

`from llama_index.llms.dashscope import DashScope`里的`DashScope`是跟`OpenAILike`类似的东西，都是LLM provider，是阿里原生Provider，不需要base_url/api_base。  

`from llama_index.core import PromptTemplate`的`PromptTemplate`可以把提示词变模版，使用它可以更新它内置的提示词模版，`query_engine.update_prompts()`来更新。
LlamaIndex 常用的 prompt 模板包括：text_qa_template、refine_template。  
上面代码用的就是`text_qa_template`。  

`text_qa_template`模版有两个参数`query_str`（问题）和`context_str`（召回内容）。

```
def load_key():
    import os
    import getpass
    import json
    import dashscope
    file_name = '../Key.json'
    if os.path.exists(file_name):
        with open(file_name, 'r') as file:
            Key = json.load(file)
        if "DASHSCOPE_API_KEY" in Key:
            os.environ['DASHSCOPE_API_KEY'] = Key["DASHSCOPE_API_KEY"].strip()
    else:
        DASHSCOPE_API_KEY = getpass.getpass("未找到存放Key的文件，请输入你的api_key:").strip()
        Key = {
            "DASHSCOPE_API_KEY": DASHSCOPE_API_KEY
        }
        # 指定文件名
        file_name = '../Key.json'
        with open(file_name, 'w') as json_file:
            json.dump(Key, json_file, indent=4)
        os.environ['DASHSCOPE_API_KEY'] = Key["DASHSCOPE_API_KEY"]
    dashscope.api_key = os.environ["DASHSCOPE_API_KEY"]
```

这是处理api的代码，值得记录一下，`getpass.getpass()`用于用户输入，好处是可以隐藏明文。
```
Key = {
  "DASHSCOPE_API_KEY": DASHSCOPE_API_KEY
}
```
`Key`的结构可以注意一下。

```
contexts = [node.get_content() for node in response.source_nodes]
contexts
```
这是查看模型召回了什么信息的代码，其中`response`它是`query_engine.query()`执行后返回的对象，同样也通过`response`输出回答，`response.print_response_stream()`。



# LlamaIndex 常见 LLM provider
从 0.13+ 开始，LlamaIndex 把大量 provider 拆成独立 pip 包（llama-index-llms-xxx）。官方也明确建议按需安装。  
**OpenAI 系列**
```
from llama_index.llms.openai import OpenAI
from llama_index.llms.openai_like import OpenAILike
```
**本地/自部署常见**
```
from llama_index.llms.ollama import Ollama 
from llama_index.llms.vllm import Vllm（vLLM client）
OpenAILike 也常用于 LocalAI / Grok 等“OpenAI-compatible 服务”
```
**其他云厂商（举例）**
这类通常是独立 integration 包，对应 llama_index.llms.<provider>（你装了才有）。LlamaIndex 生态里这类非常多（官方说 integration 包 300+）。

## 你本机“一键列出”你装了哪些 provider（推荐你用这个）
```
import pkgutil
import llama_index.llms

mods = sorted(m.name for m in pkgutil.iter_modules(llama_index.llms.__path__))
print("llama_index.llms submodules:")
for m in mods:
    print(" -", m)
```  
同理，如果你还想列 embeddings：
```
import pkgutil
import llama_index.embeddings

mods = sorted(m.name for m in pkgutil.iter_modules(llama_index.embeddings.__path__))
print("llama_index.embeddings submodules:")
for m in mods:
    print(" -", m)
```
这会给你你机器上“确实可用”的 provider 名单。
